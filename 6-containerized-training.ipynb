{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3802510a-b813-40c2-a77e-906e191df9d2",
   "metadata": {},
   "source": [
    "## Configure environment settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d80c27-6e4d-4d55-8e5c-fc8fa40020bf",
   "metadata": {},
   "source": [
    "Set location paths, connections strings, and other environment settings. Make sure to update   `REGION`, and `ARTIFACT_STORE`  with the settings reflecting your lab environment. \n",
    "\n",
    "- `REGION` - the compute region for Vertex AI Training and Prediction\n",
    "- `ARTIFACT_STORE` - A GCS bucket in the created in the same region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d55d2-f592-40aa-af7d-9926ae0fba1b",
   "metadata": {},
   "source": [
    "Containerized Training Cloud Serving\n",
    "- `Phase 1` - (Before ML Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28d72e95-efe9-450f-a629-1a53d51e4b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "497ee2f8-6173-467b-b80a-f63ea1ce0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-east1\"\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-beer-artifact-store\"\n",
    "\n",
    "DATA_ROOT = f\"{ARTIFACT_STORE}/data\"\n",
    "JOB_DIR_ROOT = f\"{ARTIFACT_STORE}/jobs\"\n",
    "TRAINING_FILE_PATH = f\"{DATA_ROOT}/train.parquet\"\n",
    "VALIDATION_FILE_PATH = f\"{DATA_ROOT}/validation/valid.parquet\"\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c4049d0-e395-4c17-8b28-011316630e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JOB_DIR_ROOT\"] = JOB_DIR_ROOT\n",
    "os.environ[\"TRAINING_FILE_PATH\"] = TRAINING_FILE_PATH\n",
    "os.environ[\"VALIDATION_FILE_PATH\"] = VALIDATION_FILE_PATH\n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cbe53f6-febd-492d-8d85-a65ee900c877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-asl-04-5e165f533cac-beer-artifact-store/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a54a91-79ad-4e10-a3cf-fd3aa1d789cf",
   "metadata": {},
   "source": [
    "### Package the script into a docker image.\n",
    "\n",
    "Notice that we are installing specific versions of `scikit-learn` and `pandas` in the training image. This is done to make sure that the training runtime in the training container is aligned with the serving runtime in the serving container. \n",
    "\n",
    "Make sure to update the URI for the base image so that it points to your project's **Container Registry**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6caa7193-c966-4aa2-a2ee-9833857b852b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70e86dde-f309-4735-84c8-12ff69d08658",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME = \"trainer_image\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "os.environ[\"IMAGE_URI\"] = IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfd37de2-dab7-45a3-b944-da4c4c7c085f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 3 file(s) totalling 12.2 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://qwiklabs-asl-04-5e165f533cac_cloudbuild/source/1654231732.212619-706f54b26dff490f83816fbb858057a2.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-asl-04-5e165f533cac/locations/global/builds/6f4f124f-d204-4cc1-aef9-7b0bd4e5efe2].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/6f4f124f-d204-4cc1-aef9-7b0bd4e5efe2?project=547029906128].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"6f4f124f-d204-4cc1-aef9-7b0bd4e5efe2\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-asl-04-5e165f533cac_cloudbuild/source/1654231732.212619-706f54b26dff490f83816fbb858057a2.tgz#1654231732422676\n",
      "Copying gs://qwiklabs-asl-04-5e165f533cac_cloudbuild/source/1654231732.212619-706f54b26dff490f83816fbb858057a2.tgz#1654231732422676...\n",
      "/ [1 files][  2.7 KiB/  2.7 KiB]                                                \n",
      "Operation completed over 1 objects/2.7 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   16.9kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d5fd17ec1767: Already exists\n",
      "adb65a4cea80: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "4ae9dce86192: Pulling fs layer\n",
      "3c97dd6fc917: Pulling fs layer\n",
      "9a421421a9c6: Pulling fs layer\n",
      "8c3aa27ae5ff: Pulling fs layer\n",
      "3c97dd6fc917: Waiting\n",
      "9a421421a9c6: Waiting\n",
      "ed3dcc4cfb6c: Pulling fs layer\n",
      "3debde04b610: Pulling fs layer\n",
      "f5164395e213: Pulling fs layer\n",
      "8c3aa27ae5ff: Waiting\n",
      "ed3dcc4cfb6c: Waiting\n",
      "3debde04b610: Waiting\n",
      "7c0e9eccc936: Pulling fs layer\n",
      "d78c60a0dcbe: Pulling fs layer\n",
      "ff2acd98ca45: Pulling fs layer\n",
      "a11a78983f17: Pulling fs layer\n",
      "6b33fb6f0d3a: Pulling fs layer\n",
      "3293f65cfcaa: Pulling fs layer\n",
      "aa9e1ad00162: Pulling fs layer\n",
      "f5164395e213: Waiting\n",
      "9c25ce1cf557: Pulling fs layer\n",
      "ff2acd98ca45: Waiting\n",
      "a11a78983f17: Waiting\n",
      "6b33fb6f0d3a: Waiting\n",
      "3293f65cfcaa: Waiting\n",
      "aa9e1ad00162: Waiting\n",
      "9c25ce1cf557: Waiting\n",
      "7c0e9eccc936: Waiting\n",
      "d78c60a0dcbe: Waiting\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "adb65a4cea80: Verifying Checksum\n",
      "adb65a4cea80: Download complete\n",
      "9a421421a9c6: Verifying Checksum\n",
      "9a421421a9c6: Download complete\n",
      "adb65a4cea80: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "8c3aa27ae5ff: Verifying Checksum\n",
      "8c3aa27ae5ff: Download complete\n",
      "ed3dcc4cfb6c: Verifying Checksum\n",
      "ed3dcc4cfb6c: Download complete\n",
      "3debde04b610: Verifying Checksum\n",
      "3debde04b610: Download complete\n",
      "f5164395e213: Verifying Checksum\n",
      "f5164395e213: Download complete\n",
      "3c97dd6fc917: Verifying Checksum\n",
      "3c97dd6fc917: Download complete\n",
      "7c0e9eccc936: Verifying Checksum\n",
      "7c0e9eccc936: Download complete\n",
      "d78c60a0dcbe: Verifying Checksum\n",
      "d78c60a0dcbe: Download complete\n",
      "ff2acd98ca45: Verifying Checksum\n",
      "ff2acd98ca45: Download complete\n",
      "a11a78983f17: Verifying Checksum\n",
      "a11a78983f17: Download complete\n",
      "6b33fb6f0d3a: Verifying Checksum\n",
      "6b33fb6f0d3a: Download complete\n",
      "3293f65cfcaa: Verifying Checksum\n",
      "3293f65cfcaa: Download complete\n",
      "9c25ce1cf557: Verifying Checksum\n",
      "9c25ce1cf557: Download complete\n",
      "4ae9dce86192: Verifying Checksum\n",
      "4ae9dce86192: Download complete\n",
      "aa9e1ad00162: Verifying Checksum\n",
      "aa9e1ad00162: Download complete\n",
      "4ae9dce86192: Pull complete\n",
      "3c97dd6fc917: Pull complete\n",
      "9a421421a9c6: Pull complete\n",
      "8c3aa27ae5ff: Pull complete\n",
      "ed3dcc4cfb6c: Pull complete\n",
      "3debde04b610: Pull complete\n",
      "f5164395e213: Pull complete\n",
      "7c0e9eccc936: Pull complete\n",
      "d78c60a0dcbe: Pull complete\n",
      "ff2acd98ca45: Pull complete\n",
      "a11a78983f17: Pull complete\n",
      "6b33fb6f0d3a: Pull complete\n",
      "3293f65cfcaa: Pull complete\n",
      "aa9e1ad00162: Pull complete\n",
      "9c25ce1cf557: Pull complete\n",
      "Digest: sha256:8ad8673b3dcf0645737cc7adf32c0bacec683532ffdeae26fde760998c262972\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 4d10005d4e6f\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune\n",
      " ---> Running in 741dbfc40a6a\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 kB 4.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=ea7bfb327e55cc2ac0db1ba2852c45edabc22beaddeb3641cd86a45a7019b403\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3987 sha256=48b5e33e46f7f2bbebbaee433b1950abfb20c23d92e34107c398c73edf6c897b\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=ceb1ca71a79dfdd94878f94800f499b764afdd1b4ed2c00fb597571e56c1e4b2\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "Installing collected packages: termcolor, cloudml-hypertune, fire\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.4.0 termcolor-1.1.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 741dbfc40a6a\n",
      " ---> 6fcea10d968e\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 595c9e74624c\n",
      "Removing intermediate container 595c9e74624c\n",
      " ---> 8c537a8ba98b\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 69e76dea4a98\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 70f056dcd347\n",
      "Removing intermediate container 70f056dcd347\n",
      " ---> a48626e5fbfc\n",
      "Successfully built a48626e5fbfc\n",
      "Successfully tagged gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image]\n",
      "54b874b0daac: Preparing\n",
      "98276a2d4756: Preparing\n",
      "151454194eab: Preparing\n",
      "a74d37ed3ebe: Preparing\n",
      "358d0c97e748: Preparing\n",
      "a72622d8d85c: Preparing\n",
      "a44ab01c5179: Preparing\n",
      "5dc3e696ff5b: Preparing\n",
      "614f43fff4a9: Preparing\n",
      "ae2d9c14a32d: Preparing\n",
      "7d7cd198ee7d: Preparing\n",
      "186b46a1d19e: Preparing\n",
      "ac6ab83c1c63: Preparing\n",
      "c8e5c31d7bf8: Preparing\n",
      "3e7b03bffb47: Preparing\n",
      "cebed4d6cdf8: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "8320c4cf558f: Preparing\n",
      "67fe02a0cfc8: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "65f26f75faab: Preparing\n",
      "bf8cedc62fb3: Preparing\n",
      "a72622d8d85c: Waiting\n",
      "a44ab01c5179: Waiting\n",
      "5dc3e696ff5b: Waiting\n",
      "614f43fff4a9: Waiting\n",
      "ae2d9c14a32d: Waiting\n",
      "7d7cd198ee7d: Waiting\n",
      "186b46a1d19e: Waiting\n",
      "ac6ab83c1c63: Waiting\n",
      "c8e5c31d7bf8: Waiting\n",
      "3e7b03bffb47: Waiting\n",
      "cebed4d6cdf8: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "8320c4cf558f: Waiting\n",
      "67fe02a0cfc8: Waiting\n",
      "65f26f75faab: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "358d0c97e748: Layer already exists\n",
      "a74d37ed3ebe: Layer already exists\n",
      "a44ab01c5179: Layer already exists\n",
      "a72622d8d85c: Layer already exists\n",
      "5dc3e696ff5b: Layer already exists\n",
      "614f43fff4a9: Layer already exists\n",
      "ae2d9c14a32d: Layer already exists\n",
      "7d7cd198ee7d: Layer already exists\n",
      "186b46a1d19e: Layer already exists\n",
      "ac6ab83c1c63: Layer already exists\n",
      "c8e5c31d7bf8: Layer already exists\n",
      "3e7b03bffb47: Layer already exists\n",
      "cebed4d6cdf8: Layer already exists\n",
      "8320c4cf558f: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "67fe02a0cfc8: Layer already exists\n",
      "65f26f75faab: Layer already exists\n",
      "bf8cedc62fb3: Layer already exists\n",
      "151454194eab: Pushed\n",
      "98276a2d4756: Pushed\n",
      "54b874b0daac: Pushed\n",
      "latest: digest: sha256:d010a205163928b4e6338cf8f4c317b46a03a69701384a8320fdca4ce88709e6 size: 4911\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                       STATUS\n",
      "6f4f124f-d204-4cc1-aef9-7b0bd4e5efe2  2022-06-03T04:48:52+00:00  1M20S     gs://qwiklabs-asl-04-5e165f533cac_cloudbuild/source/1654231732.212619-706f54b26dff490f83816fbb858057a2.tgz  gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df893958-0b4b-4427-bc40-afb59dce5e41",
   "metadata": {},
   "source": [
    "## Submit an Vertex AI hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6af82820-d836-4fce-baa8-a046adef2c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"beer_recom_tuning_{TIMESTAMP}\"\n",
    "JOB_DIR = f\"{JOB_DIR_ROOT}/{JOB_NAME}\"\n",
    "\n",
    "os.environ[\"JOB_NAME\"] = JOB_NAME\n",
    "os.environ[\"JOB_DIR\"] = JOB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "045e6257-aa09-442e-bfab-79fe39378180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME: beer_recom_tuning_20220603_045016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-east1-aiplatform.googleapis.com/]\n",
      "Hyperparameter tuning job [2329788173443399680] submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai hp-tuning-jobs describe 2329788173443399680 --region=us-east1\n",
      "\n",
      "Job State: JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "CONFIG_YAML=config.yaml\n",
    "\n",
    "gcloud ai hp-tuning-jobs create \\\n",
    "    --region=$REGION \\\n",
    "    --display-name=$JOB_NAME \\\n",
    "    --config=$CONFIG_YAML \\\n",
    "    --max-trial-count=5 \\\n",
    "    --parallel-trial-count=5\n",
    "\n",
    "echo \"JOB_NAME: $JOB_NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "449b597e-a2ef-46a9-bfb5-f5b30d5d5fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-east1-aiplatform.googleapis.com/]\n",
      "createTime: '2022-06-03T04:50:17.775809Z'\n",
      "displayName: beer_recom_tuning_20220603_045016\n",
      "maxTrialCount: 5\n",
      "name: projects/547029906128/locations/us-east1/hyperparameterTuningJobs/2329788173443399680\n",
      "parallelTrialCount: 5\n",
      "startTime: '2022-06-03T04:50:24Z'\n",
      "state: JOB_STATE_RUNNING\n",
      "studySpec:\n",
      "  metrics:\n",
      "  - goal: MAXIMIZE\n",
      "    metricId: map_at_10\n",
      "  parameters:\n",
      "  - discreteValueSpec:\n",
      "      values:\n",
      "      - 10.0\n",
      "      - 20.0\n",
      "    parameterId: iterations\n",
      "  - doubleValueSpec:\n",
      "      maxValue: 0.1\n",
      "      minValue: 0.0001\n",
      "    parameterId: regularization\n",
      "    scaleType: UNIT_LINEAR_SCALE\n",
      "trialJobSpec:\n",
      "  workerPoolSpecs:\n",
      "  - containerSpec:\n",
      "      args:\n",
      "      - --job_dir=$JOB_DIR\n",
      "      - --training_dataset_path=$TRAINING_FILE_PATH\n",
      "      - --validation_dataset_path=$VALIDATION_FILE_PATH\n",
      "      - --hptune\n",
      "      imageUri: gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image\n",
      "    diskSpec:\n",
      "      bootDiskSizeGb: 100\n",
      "      bootDiskType: pd-ssd\n",
      "    machineSpec:\n",
      "      machineType: n1-standard-4\n",
      "    replicaCount: '1'\n",
      "trials:\n",
      "- id: '1'\n",
      "  parameters:\n",
      "  - parameterId: iterations\n",
      "    value: 20\n",
      "  - parameterId: regularization\n",
      "    value: 0.05005\n",
      "  startTime: '2022-06-03T04:50:28.062876044Z'\n",
      "  state: ACTIVE\n",
      "- id: '2'\n",
      "  parameters:\n",
      "  - parameterId: iterations\n",
      "    value: 10\n",
      "  - parameterId: regularization\n",
      "    value: 0.0720269\n",
      "  startTime: '2022-06-03T04:50:28.063014785Z'\n",
      "  state: ACTIVE\n",
      "- id: '3'\n",
      "  parameters:\n",
      "  - parameterId: iterations\n",
      "    value: 10\n",
      "  - parameterId: regularization\n",
      "    value: 0.04806\n",
      "  startTime: '2022-06-03T04:50:28.063060074Z'\n",
      "  state: ACTIVE\n",
      "- id: '4'\n",
      "  parameters:\n",
      "  - parameterId: iterations\n",
      "    value: 10\n",
      "  - parameterId: regularization\n",
      "    value: 0.0976394\n",
      "  startTime: '2022-06-03T04:50:28.063093915Z'\n",
      "  state: ACTIVE\n",
      "- id: '5'\n",
      "  parameters:\n",
      "  - parameterId: iterations\n",
      "    value: 10\n",
      "  - parameterId: regularization\n",
      "    value: 0.0201387\n",
      "  startTime: '2022-06-03T04:50:28.063140195Z'\n",
      "  state: ACTIVE\n",
      "updateTime: '2022-06-03T04:53:51.149134Z'\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai hp-tuning-jobs describe 2329788173443399680 --region=us-east1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a960ae-0a31-416e-8e3f-27da59680ca7",
   "metadata": {},
   "source": [
    "### Retrieve HP-tuning results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b5986c-ea52-4548-aa23-b501d7961eb2",
   "metadata": {},
   "source": [
    "After the job completes you can review the results using GCP Console or programmatically using the following functions (note that this code supposes that the metrics that the hyperparameter tuning engine optimizes is maximized): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76ecb409-fcd2-4868-aef9-398af862f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trials(job_name):\n",
    "    jobs = aiplatform.HyperparameterTuningJob.list()\n",
    "    match = [job for job in jobs if job.display_name == JOB_NAME]\n",
    "    tuning_job = match[0] if match else None\n",
    "    return tuning_job.trials if tuning_job else None\n",
    "\n",
    "\n",
    "def get_best_trial(trials):\n",
    "    metrics = [trial.final_measurement.metrics[0].value for trial in trials]\n",
    "    best_trial = trials[metrics.index(max(metrics))]\n",
    "    return best_trial\n",
    "\n",
    "\n",
    "def retrieve_best_trial_from_job_name(jobname):\n",
    "    trials = get_trials(jobname)\n",
    "    best_trial = get_best_trial(trials)\n",
    "    return best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9ee2f10-b282-4628-a762-e73751928c0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_301096/3758979191.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_best_trial_from_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJOB_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_301096/3754359669.py\u001b[0m in \u001b[0;36mretrieve_best_trial_from_job_name\u001b[0;34m(jobname)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mretrieve_best_trial_from_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_301096/3754359669.py\u001b[0m in \u001b[0;36mget_best_trial\u001b[0;34m(trials)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_measurement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "best_trial = retrieve_best_trial_from_job_name(JOB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4eb79e-7f08-49d7-b987-b5d442058530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
