{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3802510a-b813-40c2-a77e-906e191df9d2",
   "metadata": {},
   "source": [
    "## Configure environment settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d80c27-6e4d-4d55-8e5c-fc8fa40020bf",
   "metadata": {},
   "source": [
    "Set location paths, connections strings, and other environment settings. Make sure to update   `REGION`, and `ARTIFACT_STORE`  with the settings reflecting your lab environment. \n",
    "\n",
    "- `REGION` - the compute region for Vertex AI Training and Prediction\n",
    "- `ARTIFACT_STORE` - A GCS bucket in the created in the same region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d55d2-f592-40aa-af7d-9926ae0fba1b",
   "metadata": {},
   "source": [
    "Containerized Training Cloud Serving\n",
    "- `Phase 1` - (Before ML Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28d72e95-efe9-450f-a629-1a53d51e4b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "497ee2f8-6173-467b-b80a-f63ea1ce0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-east1\"\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-beer-artifact-store\"\n",
    "\n",
    "DATA_ROOT = f\"{ARTIFACT_STORE}/data\"\n",
    "JOB_DIR_ROOT = f\"{ARTIFACT_STORE}/jobs\"\n",
    "TRAINING_FILE_PATH = f\"{DATA_ROOT}/train.parquet\"\n",
    "VALIDATION_FILE_PATH = f\"{DATA_ROOT}/valid.parquet\"\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c4049d0-e395-4c17-8b28-011316630e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JOB_DIR_ROOT\"] = JOB_DIR_ROOT\n",
    "os.environ[\"TRAINING_FILE_PATH\"] = TRAINING_FILE_PATH\n",
    "os.environ[\"VALIDATION_FILE_PATH\"] = VALIDATION_FILE_PATH\n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cbe53f6-febd-492d-8d85-a65ee900c877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-asl-04-5e165f533cac-beer-artifact-store/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a54a91-79ad-4e10-a3cf-fd3aa1d789cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Package the script into a docker image.\n",
    "\n",
    "Notice that we are installing specific versions of `scikit-learn` and `pandas` in the training image. This is done to make sure that the training runtime in the training container is aligned with the serving runtime in the serving container. \n",
    "\n",
    "Make sure to update the URI for the base image so that it points to your project's **Container Registry**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a7be472-cff7-452e-9337-1c169823ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = \"training_app\"\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6caa7193-c966-4aa2-a2ee-9833857b852b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune implicit\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70e86dde-f309-4735-84c8-12ff69d08658",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME = \"trainer_image\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "\n",
    "os.environ[\"IMAGE_URI\"] = IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfd37de2-dab7-45a3-b944-da4c4c7c085f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 4 file(s) totalling 12.3 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://qwiklabs-asl-04-5e165f533cac_cloudbuild/source/1654568701.931167-bad19bd8f3654931afdc32eb3b40a166.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-asl-04-5e165f533cac/locations/global/builds/9badeee5-c921-45e1-97cc-8ee02ccc39df].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/9badeee5-c921-45e1-97cc-8ee02ccc39df?project=547029906128].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"9badeee5-c921-45e1-97cc-8ee02ccc39df\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-asl-04-5e165f533cac_cloudbuild/source/1654568701.931167-bad19bd8f3654931afdc32eb3b40a166.tgz#1654568702179003\n",
      "Copying gs://qwiklabs-asl-04-5e165f533cac_cloudbuild/source/1654568701.931167-bad19bd8f3654931afdc32eb3b40a166.tgz#1654568702179003...\n",
      "/ [1 files][  2.7 KiB/  2.7 KiB]                                                \n",
      "Operation completed over 1 objects/2.7 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   16.9kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d5fd17ec1767: Pulling fs layer\n",
      "adb65a4cea80: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "4ae9dce86192: Pulling fs layer\n",
      "3c97dd6fc917: Pulling fs layer\n",
      "9a421421a9c6: Pulling fs layer\n",
      "8c3aa27ae5ff: Pulling fs layer\n",
      "ed3dcc4cfb6c: Pulling fs layer\n",
      "3debde04b610: Pulling fs layer\n",
      "f5164395e213: Pulling fs layer\n",
      "7c0e9eccc936: Pulling fs layer\n",
      "d78c60a0dcbe: Pulling fs layer\n",
      "ff2acd98ca45: Pulling fs layer\n",
      "a11a78983f17: Pulling fs layer\n",
      "6b33fb6f0d3a: Pulling fs layer\n",
      "3293f65cfcaa: Pulling fs layer\n",
      "aa9e1ad00162: Pulling fs layer\n",
      "9c25ce1cf557: Pulling fs layer\n",
      "3c97dd6fc917: Waiting\n",
      "9a421421a9c6: Waiting\n",
      "8c3aa27ae5ff: Waiting\n",
      "ed3dcc4cfb6c: Waiting\n",
      "3debde04b610: Waiting\n",
      "f5164395e213: Waiting\n",
      "7c0e9eccc936: Waiting\n",
      "d78c60a0dcbe: Waiting\n",
      "ff2acd98ca45: Waiting\n",
      "a11a78983f17: Waiting\n",
      "6b33fb6f0d3a: Waiting\n",
      "3293f65cfcaa: Waiting\n",
      "aa9e1ad00162: Waiting\n",
      "9c25ce1cf557: Waiting\n",
      "4ae9dce86192: Waiting\n",
      "adb65a4cea80: Verifying Checksum\n",
      "adb65a4cea80: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "d5fd17ec1767: Verifying Checksum\n",
      "d5fd17ec1767: Download complete\n",
      "9a421421a9c6: Verifying Checksum\n",
      "9a421421a9c6: Download complete\n",
      "8c3aa27ae5ff: Verifying Checksum\n",
      "8c3aa27ae5ff: Download complete\n",
      "3c97dd6fc917: Download complete\n",
      "ed3dcc4cfb6c: Verifying Checksum\n",
      "ed3dcc4cfb6c: Download complete\n",
      "3debde04b610: Verifying Checksum\n",
      "3debde04b610: Download complete\n",
      "7c0e9eccc936: Verifying Checksum\n",
      "7c0e9eccc936: Download complete\n",
      "f5164395e213: Verifying Checksum\n",
      "f5164395e213: Download complete\n",
      "ff2acd98ca45: Verifying Checksum\n",
      "ff2acd98ca45: Download complete\n",
      "d78c60a0dcbe: Verifying Checksum\n",
      "d78c60a0dcbe: Download complete\n",
      "6b33fb6f0d3a: Verifying Checksum\n",
      "6b33fb6f0d3a: Download complete\n",
      "a11a78983f17: Verifying Checksum\n",
      "a11a78983f17: Download complete\n",
      "3293f65cfcaa: Verifying Checksum\n",
      "3293f65cfcaa: Download complete\n",
      "9c25ce1cf557: Verifying Checksum\n",
      "9c25ce1cf557: Download complete\n",
      "4ae9dce86192: Verifying Checksum\n",
      "4ae9dce86192: Download complete\n",
      "d5fd17ec1767: Pull complete\n",
      "adb65a4cea80: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "aa9e1ad00162: Verifying Checksum\n",
      "aa9e1ad00162: Download complete\n",
      "4ae9dce86192: Pull complete\n",
      "3c97dd6fc917: Pull complete\n",
      "9a421421a9c6: Pull complete\n",
      "8c3aa27ae5ff: Pull complete\n",
      "ed3dcc4cfb6c: Pull complete\n",
      "3debde04b610: Pull complete\n",
      "f5164395e213: Pull complete\n",
      "7c0e9eccc936: Pull complete\n",
      "d78c60a0dcbe: Pull complete\n",
      "ff2acd98ca45: Pull complete\n",
      "a11a78983f17: Pull complete\n",
      "6b33fb6f0d3a: Pull complete\n",
      "3293f65cfcaa: Pull complete\n",
      "aa9e1ad00162: Pull complete\n",
      "9c25ce1cf557: Pull complete\n",
      "Digest: sha256:8ad8673b3dcf0645737cc7adf32c0bacec683532ffdeae26fde760998c262972\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 4d10005d4e6f\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune implicit\n",
      " ---> Running in 391f43401190\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 kB 5.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting implicit\n",
      "  Downloading implicit-0.5.2-cp37-cp37m-manylinux2014_x86_64.whl (18.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.5/18.5 MB 30.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scipy>=0.16 in /opt/conda/lib/python3.7/site-packages (from implicit) (1.7.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from implicit) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from implicit) (4.64.0)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=ba7f4fd2f6678b5a5d8ac37f9f5af1e2b76e3a0a3657d2b8b402671a616541d5\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3987 sha256=cb82a3c20aae38cfcd19b20408afbc7f5051104b3a6d6c3cccc3cf402b5660f7\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=ef0b0a14af3638ffe540b82f368aa04b1fdfe0acd50d8acd199cd72aa54bc0a2\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "Installing collected packages: termcolor, cloudml-hypertune, fire, implicit\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.4.0 implicit-0.5.2 termcolor-1.1.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 391f43401190\n",
      " ---> 88e6f399db08\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 345da2ae84bb\n",
      "Removing intermediate container 345da2ae84bb\n",
      " ---> 71cbc4e8a754\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> e9d4d2c358e9\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 987e54cd51df\n",
      "Removing intermediate container 987e54cd51df\n",
      " ---> 048a586fb7ef\n",
      "Successfully built 048a586fb7ef\n",
      "Successfully tagged gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image]\n",
      "d38cc03fbdb3: Preparing\n",
      "2b3a2d3ffab1: Preparing\n",
      "c26cdc5c8550: Preparing\n",
      "a74d37ed3ebe: Preparing\n",
      "358d0c97e748: Preparing\n",
      "a72622d8d85c: Preparing\n",
      "a44ab01c5179: Preparing\n",
      "5dc3e696ff5b: Preparing\n",
      "614f43fff4a9: Preparing\n",
      "ae2d9c14a32d: Preparing\n",
      "7d7cd198ee7d: Preparing\n",
      "186b46a1d19e: Preparing\n",
      "ac6ab83c1c63: Preparing\n",
      "c8e5c31d7bf8: Preparing\n",
      "3e7b03bffb47: Preparing\n",
      "cebed4d6cdf8: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "8320c4cf558f: Preparing\n",
      "67fe02a0cfc8: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "65f26f75faab: Preparing\n",
      "bf8cedc62fb3: Preparing\n",
      "a72622d8d85c: Waiting\n",
      "a44ab01c5179: Waiting\n",
      "5dc3e696ff5b: Waiting\n",
      "614f43fff4a9: Waiting\n",
      "ae2d9c14a32d: Waiting\n",
      "7d7cd198ee7d: Waiting\n",
      "186b46a1d19e: Waiting\n",
      "ac6ab83c1c63: Waiting\n",
      "c8e5c31d7bf8: Waiting\n",
      "3e7b03bffb47: Waiting\n",
      "cebed4d6cdf8: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "8320c4cf558f: Waiting\n",
      "67fe02a0cfc8: Waiting\n",
      "65f26f75faab: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "358d0c97e748: Layer already exists\n",
      "a74d37ed3ebe: Layer already exists\n",
      "a44ab01c5179: Layer already exists\n",
      "a72622d8d85c: Layer already exists\n",
      "614f43fff4a9: Layer already exists\n",
      "5dc3e696ff5b: Layer already exists\n",
      "ae2d9c14a32d: Layer already exists\n",
      "7d7cd198ee7d: Layer already exists\n",
      "ac6ab83c1c63: Layer already exists\n",
      "186b46a1d19e: Layer already exists\n",
      "c8e5c31d7bf8: Layer already exists\n",
      "3e7b03bffb47: Layer already exists\n",
      "cebed4d6cdf8: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "67fe02a0cfc8: Layer already exists\n",
      "8320c4cf558f: Layer already exists\n",
      "65f26f75faab: Layer already exists\n",
      "bf8cedc62fb3: Layer already exists\n",
      "d38cc03fbdb3: Pushed\n",
      "2b3a2d3ffab1: Pushed\n",
      "c26cdc5c8550: Pushed\n",
      "latest: digest: sha256:803bbd141f6addd338dd16de99a624c6679b596d2e7eb004558a848b441b3160 size: 4913\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                       STATUS\n",
      "9badeee5-c921-45e1-97cc-8ee02ccc39df  2022-06-07T02:25:02+00:00  2M14S     gs://qwiklabs-asl-04-5e165f533cac_cloudbuild/source/1654568701.931167-bad19bd8f3654931afdc32eb3b40a166.tgz  gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df893958-0b4b-4427-bc40-afb59dce5e41",
   "metadata": {},
   "source": [
    "## Submit an Vertex AI hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6af82820-d836-4fce-baa8-a046adef2c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"beer_recom_tuning_{TIMESTAMP}\"\n",
    "JOB_DIR = f\"{JOB_DIR_ROOT}/{JOB_NAME}\"\n",
    "\n",
    "os.environ[\"JOB_NAME\"] = JOB_NAME\n",
    "os.environ[\"JOB_DIR\"] = JOB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "045e6257-aa09-442e-bfab-79fe39378180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB_NAME: beer_recom_tuning_20220607_022720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-east1-aiplatform.googleapis.com/]\n",
      "Hyperparameter tuning job [3073022849447886848] submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai hp-tuning-jobs describe 3073022849447886848 --region=us-east1\n",
      "\n",
      "Job State: JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "CONFIG_YAML=config.yaml\n",
    "\n",
    "gcloud ai hp-tuning-jobs create \\\n",
    "    --region=$REGION \\\n",
    "    --display-name=$JOB_NAME \\\n",
    "    --config=$CONFIG_YAML \\\n",
    "    --max-trial-count=5 \\\n",
    "    --parallel-trial-count=5\n",
    "\n",
    "echo \"JOB_NAME: $JOB_NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "449b597e-a2ef-46a9-bfb5-f5b30d5d5fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3073022849447886848\n",
      "Using endpoint [https://us-east1-aiplatform.googleapis.com/]\n",
      "createTime: '2022-06-07T02:27:21.658327Z'\n",
      "displayName: beer_recom_tuning_20220607_022720\n",
      "endTime: '2022-06-07T02:35:37Z'\n",
      "maxTrialCount: 5\n",
      "name: projects/547029906128/locations/us-east1/hyperparameterTuningJobs/3073022849447886848\n",
      "parallelTrialCount: 5\n",
      "startTime: '2022-06-07T02:27:28Z'\n",
      "state: JOB_STATE_SUCCEEDED\n",
      "studySpec:\n",
      "  metrics:\n",
      "  - goal: MAXIMIZE\n",
      "    metricId: map_at_10\n",
      "  parameters:\n",
      "  - discreteValueSpec:\n",
      "      values:\n",
      "      - 16.0\n",
      "      - 32.0\n",
      "      - 64.0\n",
      "      - 128.0\n",
      "    parameterId: factors\n",
      "  - integerValueSpec:\n",
      "      maxValue: '100'\n",
      "      minValue: '10'\n",
      "    parameterId: iterations\n",
      "    scaleType: UNIT_LINEAR_SCALE\n",
      "  - doubleValueSpec:\n",
      "      maxValue: 0.1\n",
      "      minValue: 0.0001\n",
      "    parameterId: regularization\n",
      "    scaleType: UNIT_LOG_SCALE\n",
      "trialJobSpec:\n",
      "  workerPoolSpecs:\n",
      "  - containerSpec:\n",
      "      imageUri: gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image\n",
      "    diskSpec:\n",
      "      bootDiskSizeGb: 100\n",
      "      bootDiskType: pd-ssd\n",
      "    machineSpec:\n",
      "      machineType: n1-standard-16\n",
      "    replicaCount: '1'\n",
      "trials:\n",
      "- endTime: '2022-06-07T02:31:43Z'\n",
      "  finalMeasurement:\n",
      "    metrics:\n",
      "    - metricId: map_at_10\n",
      "      value: 0.0826093\n",
      "    stepCount: '1'\n",
      "  id: '1'\n",
      "  parameters:\n",
      "  - parameterId: factors\n",
      "    value: 64\n",
      "  - parameterId: iterations\n",
      "    value: 55\n",
      "  - parameterId: regularization\n",
      "    value: 0.00316228\n",
      "  startTime: '2022-06-07T02:27:33.409994994Z'\n",
      "  state: SUCCEEDED\n",
      "- endTime: '2022-06-07T02:31:10Z'\n",
      "  finalMeasurement:\n",
      "    metrics:\n",
      "    - metricId: map_at_10\n",
      "      value: 0.0828859\n",
      "    stepCount: '1'\n",
      "  id: '2'\n",
      "  parameters:\n",
      "  - parameterId: factors\n",
      "    value: 64\n",
      "  - parameterId: iterations\n",
      "    value: 36\n",
      "  - parameterId: regularization\n",
      "    value: 0.000719824\n",
      "  startTime: '2022-06-07T02:27:33.410204784Z'\n",
      "  state: SUCCEEDED\n",
      "- endTime: '2022-06-07T02:31:40Z'\n",
      "  finalMeasurement:\n",
      "    metrics:\n",
      "    - metricId: map_at_10\n",
      "      value: 0.0855715\n",
      "    stepCount: '1'\n",
      "  id: '3'\n",
      "  parameters:\n",
      "  - parameterId: factors\n",
      "    value: 32\n",
      "  - parameterId: iterations\n",
      "    value: 76\n",
      "  - parameterId: regularization\n",
      "    value: 0.00203039\n",
      "  startTime: '2022-06-07T02:27:33.410279612Z'\n",
      "  state: SUCCEEDED\n",
      "- endTime: '2022-06-07T02:31:44Z'\n",
      "  finalMeasurement:\n",
      "    metrics:\n",
      "    - metricId: map_at_10\n",
      "      value: 0.0824627\n",
      "    stepCount: '1'\n",
      "  id: '4'\n",
      "  parameters:\n",
      "  - parameterId: factors\n",
      "    value: 64\n",
      "  - parameterId: iterations\n",
      "    value: 59\n",
      "  - parameterId: regularization\n",
      "    value: 0.000133693\n",
      "  startTime: '2022-06-07T02:27:33.410325611Z'\n",
      "  state: SUCCEEDED\n",
      "- endTime: '2022-06-07T02:31:45Z'\n",
      "  finalMeasurement:\n",
      "    metrics:\n",
      "    - metricId: map_at_10\n",
      "      value: 0.0857139\n",
      "    stepCount: '1'\n",
      "  id: '5'\n",
      "  parameters:\n",
      "  - parameterId: factors\n",
      "    value: 32\n",
      "  - parameterId: iterations\n",
      "    value: 100\n",
      "  - parameterId: regularization\n",
      "    value: 0.000793008\n",
      "  startTime: '2022-06-07T02:27:33.410365532Z'\n",
      "  state: SUCCEEDED\n",
      "updateTime: '2022-06-07T02:35:56.088658Z'\n"
     ]
    }
   ],
   "source": [
    "jobs = aiplatform.HyperparameterTuningJob.list()\n",
    "match = [job for job in jobs if job.display_name == JOB_NAME]\n",
    "tuning_job = match[0] if match else None\n",
    "JOB_NUM = str(tuning_job)[-19:]\n",
    "print(JOB_NUM)\n",
    "!gcloud ai hp-tuning-jobs describe $JOB_NUM --region=us-east1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a960ae-0a31-416e-8e3f-27da59680ca7",
   "metadata": {},
   "source": [
    "### Retrieve HP-tuning results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b5986c-ea52-4548-aa23-b501d7961eb2",
   "metadata": {},
   "source": [
    "After the job completes you can review the results using GCP Console or programmatically using the following functions (note that this code supposes that the metrics that the hyperparameter tuning engine optimizes is maximized): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "76ecb409-fcd2-4868-aef9-398af862f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trials(job_name):\n",
    "    #aiplatform.init(location=\"us-east1\")\n",
    "    jobs = aiplatform.HyperparameterTuningJob.list()\n",
    "    match = [job for job in jobs if job.display_name == JOB_NAME]\n",
    "    tuning_job = match[0] if match else None\n",
    "    return tuning_job.trials if tuning_job else None\n",
    "\n",
    "\n",
    "def get_best_trial(trials):\n",
    "    metrics = [trial.final_measurement.metrics[0].value for trial in trials]\n",
    "    best_trial = trials[metrics.index(max(metrics))]\n",
    "    return best_trial\n",
    "\n",
    "\n",
    "def retrieve_best_trial_from_job_name(jobname):\n",
    "    trials = get_trials(jobname)\n",
    "    best_trial = get_best_trial(trials)\n",
    "    return best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b9ee2f10-b282-4628-a762-e73751928c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: \"5\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"factors\"\n",
      "  value {\n",
      "    number_value: 32.0\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"iterations\"\n",
      "  value {\n",
      "    number_value: 100.0\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"regularization\"\n",
      "  value {\n",
      "    number_value: 0.0007930078893541997\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 1\n",
      "  metrics {\n",
      "    metric_id: \"map_at_10\"\n",
      "    value: 0.08571391392946995\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1654568853\n",
      "  nanos: 410365532\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1654569105\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_trial = retrieve_best_trial_from_job_name(JOB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8251541-bf28-4f19-a547-017c53256a24",
   "metadata": {},
   "source": [
    "## Retrain the model with the best hyperparameters\n",
    "\n",
    "You can now retrain the model using the best hyperparameters and using combined training and validation splits as a training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048550b7-20c7-4f1b-be7b-7af1aa50c839",
   "metadata": {},
   "source": [
    "### Configure and run the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1310407e-4d70-43b6-a549-096d0407f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTORS = best_trial.parameters[0].value\n",
    "ITERATIONS = best_trial.parameters[1].value\n",
    "REGULARIZATION = best_trial.parameters[2].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a7f54d14-ad9d-4d94-ac6e-b4ff9e4bb563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-east1-aiplatform.googleapis.com/]\n",
      "CustomJob [projects/547029906128/locations/us-east1/customJobs/8367004211421904896] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/547029906128/locations/us-east1/customJobs/8367004211421904896\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/547029906128/locations/us-east1/customJobs/8367004211421904896\n",
      "The model will be exported at: gs://qwiklabs-asl-04-5e165f533cac-beer-artifact-store/jobs/JOB_VERTEX_20220607_091530\n"
     ]
    }
   ],
   "source": [
    "FACTORS = best_trial.parameters[0].value\n",
    "ITERATIONS = best_trial.parameters[1].value\n",
    "REGULARIZATION = best_trial.parameters[2].value\n",
    "ISTUNE = False\n",
    "\n",
    "REGION = \"us-east1\"\n",
    "PROJECT_ID = \"qwiklabs-asl-04-5e165f533cac\"\n",
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-beer-artifact-store\"\n",
    "DATA_ROOT = os.path.join(ARTIFACT_STORE, \"data\")\n",
    "JOB_DIR_ROOT = os.path.join(ARTIFACT_STORE, \"jobs\")\n",
    "\n",
    "TIMESTAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME = f\"JOB_VERTEX_{TIMESTAMP}\"\n",
    "JOB_DIR = f\"{JOB_DIR_ROOT}/{JOB_NAME}\"\n",
    "\n",
    "MACHINE_TYPE=\"n1-standard-16\"\n",
    "REPLICA_COUNT=1\n",
    "\n",
    "WORKER_POOL_SPEC = f\"\"\"\\\n",
    "machine-type={MACHINE_TYPE},\\\n",
    "replica-count={REPLICA_COUNT},\\\n",
    "container-image-uri={IMAGE_URI}\\\n",
    "\"\"\"\n",
    "\n",
    "ARGS = f\"\"\"\\\n",
    "--factors={FACTORS},\\\n",
    "--iterations={ITERATIONS},\\\n",
    "--regularization={REGULARIZATION},\\\n",
    "--is_tune={ISTUNE}\n",
    "\"\"\"\n",
    "\n",
    "!gcloud ai custom-jobs create \\\n",
    "  --region={REGION} \\\n",
    "  --display-name={JOB_NAME} \\\n",
    "  --worker-pool-spec={WORKER_POOL_SPEC} \\\n",
    "  --args={ARGS}\n",
    "\n",
    "#!gcloud ai custom-jobs create \\\n",
    "#  --region={REGION} \\\n",
    "#  --display-name={JOB_NAME} \\\n",
    "#  --worker-pool-spec={WORKER_POOL_SPEC} \\\n",
    "\n",
    "print(\"The model will be exported at:\", JOB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "49f91fb1-0817-49e8-b167-9a084686b4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8367004211421904896\n",
      "Using endpoint [https://us-east1-aiplatform.googleapis.com/]\n",
      "createTime: '2022-06-07T09:15:31.900820Z'\n",
      "displayName: JOB_VERTEX_20220607_091530\n",
      "jobSpec:\n",
      "  workerPoolSpecs:\n",
      "  - containerSpec:\n",
      "      args:\n",
      "      - --factors=32.0\n",
      "      - --iterations=100.0\n",
      "      - --regularization=0.0007930078893541997\n",
      "      - --is_tune=False\n",
      "      imageUri: gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image:latest\n",
      "    diskSpec:\n",
      "      bootDiskSizeGb: 100\n",
      "      bootDiskType: pd-ssd\n",
      "    machineSpec:\n",
      "      machineType: n1-standard-16\n",
      "    replicaCount: '1'\n",
      "name: projects/547029906128/locations/us-east1/customJobs/8367004211421904896\n",
      "startTime: '2022-06-07T09:15:32.190339Z'\n",
      "state: JOB_STATE_PENDING\n",
      "updateTime: '2022-06-07T09:15:32.524134Z'\n"
     ]
    }
   ],
   "source": [
    "#jobs = aiplatform.HyperparameterTuningJob.list()\n",
    "jobs = aiplatform.CustomJob.list()\n",
    "match = [job for job in jobs if job.display_name == JOB_NAME]\n",
    "tuning_job = match[0] if match else None\n",
    "JOB_NUM = str(tuning_job)[-19:]\n",
    "print(JOB_NUM)\n",
    "!gcloud ai custom-jobs describe projects/547029906128/locations/us-east1/customJobs/$JOB_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7494347-fe21-4076-a9d9-bd45790853c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
