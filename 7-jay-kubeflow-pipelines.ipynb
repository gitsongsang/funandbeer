{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523f5d9a-a70e-4a16-90d0-91c51c7b4da5",
   "metadata": {},
   "source": [
    "# Continuous Training with Kubeflow Pipeline and Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a1c99-c382-4dba-90d7-df04a008b968",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "1. Learn how to use KF pre-built components\n",
    "1. Learn how to build a KF pipeline with these components\n",
    "1. Learn how to compile, upload, and run a KF pipeline\n",
    "\n",
    "\n",
    "In this lab, you will build, deploy, and run a KFP pipeline that orchestrates the **Vertex AI** services to train, tune, and deploy a **scikit-learn** model using the Google pre-built components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294032b0-960b-4895-8562-0e570ec80028",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9ed44d-c3c1-4c1f-8e40-80f7ede127ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "aiplatform.init(location=\"us-east1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01daf2eb-8ada-4b22-989e-d2f0aa6a574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"us-east1\"\n",
    "PROJECT_ID = !(gcloud config get-value project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dbec42a-6555-4880-99a5-9b72975c8971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/home/jupyter/.local/bin:/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin\n"
     ]
    }
   ],
   "source": [
    "# Set `PATH` to include the directory containing KFP CLI\n",
    "PATH = %env PATH\n",
    "%env PATH=/home/jupyter/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7003bc85-bba3-48b0-b805-a4e87f385580",
   "metadata": {},
   "source": [
    "## Understanding the pipeline design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b2a77-efc7-442a-97c0-89d78c69311c",
   "metadata": {},
   "source": [
    "The workflow implemented by the pipeline is defined using a Python based Domain Specific Language (DSL). The pipeline's DSL is in the `pipeline_vertex/pipeline_prebuilt.py` file that we will generate below.\n",
    "\n",
    "The pipeline's DSL has been designed to avoid hardcoding any environment specific settings like file paths or connection strings. These settings are provided to the pipeline code through a set of environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b4abd6-5811-414a-8ddb-9d6a1040692a",
   "metadata": {},
   "source": [
    "### Build the trainer image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "826f5670-f989-40f4-9f8e-343cc23c5aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_APP_FOLDER = \"training_app\"\n",
    "os.makedirs(TRAINING_APP_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ccb1d9d-d0a0-4c91-8e6b-2502d002854b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_app/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINING_APP_FOLDER}/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune implicit\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c422fb-4d3a-4e25-a85c-92372c244572",
   "metadata": {},
   "source": [
    "Let's now build and push this trainer container to the container registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c2799de-f951-4efc-9aa7-6089c593d42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image_beer_vertex:latest'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_NAME = \"trainer_image_beer_vertex\"\n",
    "TAG = \"latest\"\n",
    "TRAINING_CONTAINER_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\"\n",
    "TRAINING_CONTAINER_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ee89701-2249-4c7c-8168-f63435ce750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 4 file(s) totalling 16.3 KiB before compression.\n",
      "Uploading tarball of [training_app] to [gs://qwiklabs-asl-04-5e165f533cac_cloudbuild/source/1654731775.693158-5acbccaf0adf4994b66397225c6b0367.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-asl-04-5e165f533cac/locations/global/builds/6955d136-5436-429e-8408-97b86b05f64c].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/6955d136-5436-429e-8408-97b86b05f64c?project=547029906128].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"6955d136-5436-429e-8408-97b86b05f64c\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-asl-04-5e165f533cac_cloudbuild/source/1654731775.693158-5acbccaf0adf4994b66397225c6b0367.tgz#1654731775998777\n",
      "Copying gs://qwiklabs-asl-04-5e165f533cac_cloudbuild/source/1654731775.693158-5acbccaf0adf4994b66397225c6b0367.tgz#1654731775998777...\n",
      "/ [1 files][  3.2 KiB/  3.2 KiB]                                                \n",
      "Operation completed over 1 objects/3.2 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  22.02kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "d5fd17ec1767: Pulling fs layer\n",
      "adb65a4cea80: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "4ae9dce86192: Pulling fs layer\n",
      "3c97dd6fc917: Pulling fs layer\n",
      "9a421421a9c6: Pulling fs layer\n",
      "8c3aa27ae5ff: Pulling fs layer\n",
      "ed3dcc4cfb6c: Pulling fs layer\n",
      "3debde04b610: Pulling fs layer\n",
      "f5164395e213: Pulling fs layer\n",
      "7c0e9eccc936: Pulling fs layer\n",
      "d78c60a0dcbe: Pulling fs layer\n",
      "ff2acd98ca45: Pulling fs layer\n",
      "a11a78983f17: Pulling fs layer\n",
      "6b33fb6f0d3a: Pulling fs layer\n",
      "3293f65cfcaa: Pulling fs layer\n",
      "aa9e1ad00162: Pulling fs layer\n",
      "9c25ce1cf557: Pulling fs layer\n",
      "4ae9dce86192: Waiting\n",
      "3c97dd6fc917: Waiting\n",
      "9a421421a9c6: Waiting\n",
      "8c3aa27ae5ff: Waiting\n",
      "ed3dcc4cfb6c: Waiting\n",
      "3debde04b610: Waiting\n",
      "f5164395e213: Waiting\n",
      "7c0e9eccc936: Waiting\n",
      "d78c60a0dcbe: Waiting\n",
      "ff2acd98ca45: Waiting\n",
      "a11a78983f17: Waiting\n",
      "6b33fb6f0d3a: Waiting\n",
      "3293f65cfcaa: Waiting\n",
      "aa9e1ad00162: Waiting\n",
      "9c25ce1cf557: Waiting\n",
      "adb65a4cea80: Verifying Checksum\n",
      "adb65a4cea80: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "d5fd17ec1767: Verifying Checksum\n",
      "d5fd17ec1767: Download complete\n",
      "9a421421a9c6: Verifying Checksum\n",
      "9a421421a9c6: Download complete\n",
      "8c3aa27ae5ff: Verifying Checksum\n",
      "8c3aa27ae5ff: Download complete\n",
      "3c97dd6fc917: Verifying Checksum\n",
      "3c97dd6fc917: Download complete\n",
      "ed3dcc4cfb6c: Verifying Checksum\n",
      "ed3dcc4cfb6c: Download complete\n",
      "3debde04b610: Verifying Checksum\n",
      "3debde04b610: Download complete\n",
      "f5164395e213: Verifying Checksum\n",
      "f5164395e213: Download complete\n",
      "7c0e9eccc936: Verifying Checksum\n",
      "7c0e9eccc936: Download complete\n",
      "d78c60a0dcbe: Verifying Checksum\n",
      "d78c60a0dcbe: Download complete\n",
      "ff2acd98ca45: Verifying Checksum\n",
      "ff2acd98ca45: Download complete\n",
      "a11a78983f17: Verifying Checksum\n",
      "a11a78983f17: Download complete\n",
      "6b33fb6f0d3a: Verifying Checksum\n",
      "6b33fb6f0d3a: Download complete\n",
      "3293f65cfcaa: Verifying Checksum\n",
      "3293f65cfcaa: Download complete\n",
      "9c25ce1cf557: Verifying Checksum\n",
      "9c25ce1cf557: Download complete\n",
      "4ae9dce86192: Verifying Checksum\n",
      "4ae9dce86192: Download complete\n",
      "d5fd17ec1767: Pull complete\n",
      "adb65a4cea80: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "aa9e1ad00162: Verifying Checksum\n",
      "aa9e1ad00162: Download complete\n",
      "4ae9dce86192: Pull complete\n",
      "3c97dd6fc917: Pull complete\n",
      "9a421421a9c6: Pull complete\n",
      "8c3aa27ae5ff: Pull complete\n",
      "ed3dcc4cfb6c: Pull complete\n",
      "3debde04b610: Pull complete\n",
      "f5164395e213: Pull complete\n",
      "7c0e9eccc936: Pull complete\n",
      "d78c60a0dcbe: Pull complete\n",
      "ff2acd98ca45: Pull complete\n",
      "a11a78983f17: Pull complete\n",
      "6b33fb6f0d3a: Pull complete\n",
      "3293f65cfcaa: Pull complete\n",
      "aa9e1ad00162: Pull complete\n",
      "9c25ce1cf557: Pull complete\n",
      "Digest: sha256:8ad8673b3dcf0645737cc7adf32c0bacec683532ffdeae26fde760998c262972\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 4d10005d4e6f\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune implicit\n",
      " ---> Running in 6c1b8b1ff089\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 kB 6.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting implicit\n",
      "  Downloading implicit-0.5.2-cp37-cp37m-manylinux2014_x86_64.whl (18.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.5/18.5 MB 39.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from implicit) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.16 in /opt/conda/lib/python3.7/site-packages (from implicit) (1.7.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from implicit) (4.64.0)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=21f569c5b80264cffaa3bc0b4cbe804591bdf400090b7991e61ae2dea605292e\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3987 sha256=0b77081253918fb29835e3957c54ba34d7c33bf1e1e4039ade18153617154fa8\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=98a146a25647c9b203ea7fd62a98fe387884e390c02d39c015ade6788b61d496\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "Installing collected packages: termcolor, cloudml-hypertune, fire, implicit\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.4.0 implicit-0.5.2 termcolor-1.1.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 6c1b8b1ff089\n",
      " ---> 49c4bef7040b\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in fe4f88942eaf\n",
      "Removing intermediate container fe4f88942eaf\n",
      " ---> 6452601ca0fe\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> a8975fc55dce\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in d27761ec501b\n",
      "Removing intermediate container d27761ec501b\n",
      " ---> 65a530659d14\n",
      "Successfully built 65a530659d14\n",
      "Successfully tagged gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image_beer_vertex:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image_beer_vertex:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image_beer_vertex]\n",
      "3a040b961b8b: Preparing\n",
      "3dbec4c530aa: Preparing\n",
      "ed791564d4d3: Preparing\n",
      "a74d37ed3ebe: Preparing\n",
      "358d0c97e748: Preparing\n",
      "a72622d8d85c: Preparing\n",
      "a44ab01c5179: Preparing\n",
      "5dc3e696ff5b: Preparing\n",
      "614f43fff4a9: Preparing\n",
      "ae2d9c14a32d: Preparing\n",
      "7d7cd198ee7d: Preparing\n",
      "186b46a1d19e: Preparing\n",
      "ac6ab83c1c63: Preparing\n",
      "c8e5c31d7bf8: Preparing\n",
      "3e7b03bffb47: Preparing\n",
      "cebed4d6cdf8: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "8320c4cf558f: Preparing\n",
      "67fe02a0cfc8: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "65f26f75faab: Preparing\n",
      "bf8cedc62fb3: Preparing\n",
      "a72622d8d85c: Waiting\n",
      "a44ab01c5179: Waiting\n",
      "5dc3e696ff5b: Waiting\n",
      "614f43fff4a9: Waiting\n",
      "ae2d9c14a32d: Waiting\n",
      "7d7cd198ee7d: Waiting\n",
      "186b46a1d19e: Waiting\n",
      "ac6ab83c1c63: Waiting\n",
      "c8e5c31d7bf8: Waiting\n",
      "3e7b03bffb47: Waiting\n",
      "cebed4d6cdf8: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "8320c4cf558f: Waiting\n",
      "67fe02a0cfc8: Waiting\n",
      "65f26f75faab: Waiting\n",
      "bf8cedc62fb3: Waiting\n",
      "358d0c97e748: Layer already exists\n",
      "a74d37ed3ebe: Layer already exists\n",
      "a72622d8d85c: Layer already exists\n",
      "a44ab01c5179: Layer already exists\n",
      "5dc3e696ff5b: Layer already exists\n",
      "614f43fff4a9: Layer already exists\n",
      "7d7cd198ee7d: Layer already exists\n",
      "ae2d9c14a32d: Layer already exists\n",
      "ac6ab83c1c63: Layer already exists\n",
      "186b46a1d19e: Layer already exists\n",
      "c8e5c31d7bf8: Layer already exists\n",
      "3e7b03bffb47: Layer already exists\n",
      "cebed4d6cdf8: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "8320c4cf558f: Layer already exists\n",
      "67fe02a0cfc8: Layer already exists\n",
      "65f26f75faab: Layer already exists\n",
      "bf8cedc62fb3: Layer already exists\n",
      "3dbec4c530aa: Pushed\n",
      "3a040b961b8b: Pushed\n",
      "ed791564d4d3: Pushed\n",
      "latest: digest: sha256:91c4dd454d19c3eca301b1317e33aa0bd43cccf6766d56a10d04202086c17cdb size: 4913\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                                   STATUS\n",
      "6955d136-5436-429e-8408-97b86b05f64c  2022-06-08T23:42:56+00:00  1M49S     gs://qwiklabs-asl-04-5e165f533cac_cloudbuild/source/1654731775.693158-5acbccaf0adf4994b66397225c6b0367.tgz  gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image_beer_vertex (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $TRAINING_CONTAINER_IMAGE_URI $TRAINING_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c3af4c-0d70-4aec-9003-de8b15381f5d",
   "metadata": {},
   "source": [
    "## Building and deploying the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0965a289-1205-4e97-85c3-07c6501404eb",
   "metadata": {},
   "source": [
    "Let us write the pipeline to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f4da532-ca10-4c49-b874-30ef0d0324ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./jay-pipeline/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./jay-pipeline/pipeline.py\n",
    "# Copyright 2021 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n",
    "# use this file except in compliance with the License. You may obtain a copy of\n",
    "# the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\"\n",
    "# BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "# express or implied. See the License for the specific language governing\n",
    "# permissions and limitations under the License.\n",
    "import os\n",
    "\n",
    "from kfp import dsl\n",
    "from training_lightweight_component import train_and_deploy\n",
    "from tuning_lightweight_component import tune_hyperparameters\n",
    "\n",
    "PIPELINE_ROOT = os.getenv(\"PIPELINE_ROOT\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "REGION = os.getenv(\"REGION\")\n",
    "\n",
    "TRAINING_CONTAINER_IMAGE_URI = os.getenv(\"TRAINING_CONTAINER_IMAGE_URI\")\n",
    "\n",
    "TRAINING_FILE_PATH = os.getenv(\"TRAINING_FILE_PATH\")\n",
    "VALIDATION_FILE_PATH = os.getenv(\"VALIDATION_FILE_PATH\")\n",
    "\n",
    "MAX_TRIAL_COUNT = int(os.getenv(\"MAX_TRIAL_COUNT\", \"5\"))\n",
    "PARALLEL_TRIAL_COUNT = int(os.getenv(\"PARALLEL_TRIAL_COUNT\", \"5\"))\n",
    "THRESHOLD = float(os.getenv(\"THRESHOLD\", \"0.6\"))\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"beer-kfp-pipeline\",\n",
    "    description=\"The pipeline training and deploying the Beer recommandation\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def beer_recom_train(\n",
    "    training_container_uri: str = TRAINING_CONTAINER_IMAGE_URI,\n",
    "    training_file_path: str = TRAINING_FILE_PATH,\n",
    "    validation_file_path: str = VALIDATION_FILE_PATH,\n",
    "    map_at_10_deployment_threshold: float = THRESHOLD,\n",
    "    max_trial_count: int = MAX_TRIAL_COUNT,\n",
    "    parallel_trial_count: int = PARALLEL_TRIAL_COUNT,\n",
    "    pipeline_root: str = PIPELINE_ROOT,\n",
    "):\n",
    "    staging_bucket = f\"{pipeline_root}/staging\"\n",
    "\n",
    "    tuning_op = tune_hyperparameters(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        container_uri=training_container_uri,\n",
    "        training_file_path=training_file_path,\n",
    "        validation_file_path=validation_file_path,\n",
    "        staging_bucket=staging_bucket,\n",
    "        max_trial_count=max_trial_count,\n",
    "        parallel_trial_count=parallel_trial_count,\n",
    "    )\n",
    "\n",
    "    map_at_10 = tuning_op.outputs[\"best_map_at_10\"]\n",
    "\n",
    "    with dsl.Condition(\n",
    "        map_at_10 >= map_at_10_deployment_threshold, name=\"deploy_decision\"\n",
    "    ):\n",
    "        train_and_deploy_op = (  # pylint: disable=unused-variable\n",
    "            train_and_deploy(\n",
    "                project=PROJECT_ID,\n",
    "                location=REGION,\n",
    "                container_uri=training_container_uri,\n",
    "                training_file_path=training_file_path,\n",
    "                validation_file_path=validation_file_path,\n",
    "                staging_bucket=staging_bucket,\n",
    "                factors=tuning_op.outputs[\"best_factors\"],\n",
    "                regularization=tuning_op.outputs[\"best_regularization\"],\n",
    "                iterations=tuning_op.outputs[\"best_iterations\"],\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cee1d48-6b53-4b3c-b655-355aaeed9c6b",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94fefb4-105d-45ab-94f5-3181e81196a1",
   "metadata": {},
   "source": [
    "Let stat by defining the environment variables that will be passed to the pipeline compiler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52452a1c-805c-4cfc-a6df-9dcb9585e67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PIPELINE_ROOT=gs://qwiklabs-asl-04-5e165f533cac-beer-kfp-artifact-store/pipeline\n",
      "env: PROJECT_ID=qwiklabs-asl-04-5e165f533cac\n",
      "env: REGION=us-east1\n",
      "env: TRAINING_CONTAINER_IMAGE_URI=gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image_beer_vertex:latest\n",
      "env: TRAINING_FILE_PATH=gs://qwiklabs-asl-04-5e165f533cac-beer-kfp-artifact-store/data/train.parquet\n",
      "env: VALIDATION_FILE_PATH=gs://qwiklabs-asl-04-5e165f533cac-beer-kfp-artifact-store/data/valid.parquet\n"
     ]
    }
   ],
   "source": [
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-beer-artifact-store\"\n",
    "PIPELINE_ROOT = f\"{ARTIFACT_STORE}/pipeline\"\n",
    "DATA_ROOT = f\"{ARTIFACT_STORE}/data\"\n",
    "\n",
    "TRAINING_FILE_PATH = f\"{DATA_ROOT}/train.parquet\"\n",
    "VALIDATION_FILE_PATH = f\"{DATA_ROOT}/valid.parquet\"\n",
    "\n",
    "%env PIPELINE_ROOT={PIPELINE_ROOT}\n",
    "%env PROJECT_ID={PROJECT_ID}\n",
    "%env REGION={REGION}\n",
    "%env TRAINING_CONTAINER_IMAGE_URI={TRAINING_CONTAINER_IMAGE_URI}\n",
    "%env TRAINING_FILE_PATH={TRAINING_FILE_PATH}\n",
    "%env VALIDATION_FILE_PATH={VALIDATION_FILE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d0f43-47e1-401e-9286-bed8420a93a7",
   "metadata": {},
   "source": [
    "Let us make sure that the `ARTIFACT_STORE` has been created, and let us create it if not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6bdb944-6a7c-4718-bffc-c3490657f70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-asl-04-5e165f533cac-beer-kfp-artifact-store/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls | grep ^{ARTIFACT_STORE}/$ || gsutil mb -l {REGION} {ARTIFACT_STORE}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a5cd03-a4bd-4524-9ff1-7fa948c28155",
   "metadata": {},
   "source": [
    "#### Use the CLI compiler to compile the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c508395-c202-4adb-a594-06a2682d97b0",
   "metadata": {},
   "source": [
    "We compile the pipeline from the Python file we generated into a JSON description using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3ab6b72-b97d-4463-a80a-43cbb5bd89d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_JSON = \"beer_kfp_pipeline.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9a5125b-7260-407c-b1de-70da53be0216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PIPELINE_ROOT=gs://qwiklabs-asl-04-5e165f533cac-beer-kfp-artifact-store/pipeline\n",
      "env: PROJECT_ID=qwiklabs-asl-04-5e165f533cac\n",
      "env: REGION=us-east1\n",
      "env: TRAINING_CONTAINER_IMAGE_URI=gcr.io/qwiklabs-asl-04-5e165f533cac/trainer_image_beer_vertex:latest\n",
      "env: TRAINING_FILE_PATH=gs://qwiklabs-asl-04-5e165f533cac-beer-kfp-artifact-store/data/train.parquet\n",
      "env: VALIDATION_FILE_PATH=gs://qwiklabs-asl-04-5e165f533cac-beer-kfp-artifact-store/data/valid.parquet\n"
     ]
    }
   ],
   "source": [
    "ARTIFACT_STORE = f\"gs://{PROJECT_ID}-beer-kfp-artifact-store\"\n",
    "PIPELINE_ROOT = f\"{ARTIFACT_STORE}/pipeline\"\n",
    "DATA_ROOT = f\"{ARTIFACT_STORE}/data\"\n",
    "REGION = \"us-east1\"\n",
    "TRAINING_FILE_PATH = f\"{DATA_ROOT}/train.parquet\"\n",
    "VALIDATION_FILE_PATH = f\"{DATA_ROOT}/valid.parquet\"\n",
    "\n",
    "%env PIPELINE_ROOT={PIPELINE_ROOT}\n",
    "%env PROJECT_ID={PROJECT_ID}\n",
    "%env REGION={REGION}\n",
    "%env TRAINING_CONTAINER_IMAGE_URI={TRAINING_CONTAINER_IMAGE_URI}\n",
    "%env TRAINING_FILE_PATH={TRAINING_FILE_PATH}\n",
    "%env VALIDATION_FILE_PATH={VALIDATION_FILE_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb55f217-f938-4c68-a1b1-b339c4fb3dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile-v2 --py jay-pipeline/pipeline_prebuilt.py --output $PIPELINE_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44da431-9231-4f5c-98d6-26ca31036d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {PIPELINE_JSON}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9186a6fd-6431-4683-84af-d4983a9764b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name='beer_kfp_pipeline',\n",
    "    template_path=PIPELINE_JSON, \n",
    "    enable_caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8dee358f-a3ef-43a4-8443-ca00451db588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/547029906128/locations/us-east1/pipelineJobs/beer-kfp-pipeline-20220609003235\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/547029906128/locations/us-east1/pipelineJobs/beer-kfp-pipeline-20220609003235')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-east1/pipelines/runs/beer-kfp-pipeline-20220609003235?project=547029906128\n",
      "PipelineJob projects/547029906128/locations/us-east1/pipelineJobs/beer-kfp-pipeline-20220609003235 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/547029906128/locations/us-east1/pipelineJobs/beer-kfp-pipeline-20220609003235 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/547029906128/locations/us-east1/pipelineJobs/beer-kfp-pipeline-20220609003235 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/547029906128/locations/us-east1/pipelineJobs/beer-kfp-pipeline-20220609003235 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [hyperparameter-tuning-job].; Job (project_id = qwiklabs-asl-04-5e165f533cac, job_id = 6791173151377063936) is failed due to the above error.; Failed to handle the job: {project_number = 547029906128, job_id = 6791173151377063936}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2603940/3415392493.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, service_account, network, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    267\u001b[0m         )\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     def submit(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;31m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_PIPELINE_ERROR_STATES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job failed with:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_completed_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [hyperparameter-tuning-job].; Job (project_id = qwiklabs-asl-04-5e165f533cac, job_id = 6791173151377063936) is failed due to the above error.; Failed to handle the job: {project_number = 547029906128, job_id = 6791173151377063936}\"\n"
     ]
    }
   ],
   "source": [
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c7f9cb5f-165e-4850-9fc9-f24c58708480",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIE_COUNTS = 2\n",
    "\n",
    "movies = [[1,10],[2,150]]\n",
    "\n",
    "tickets = [[11,1,7],[12,2,100],[13,2,20],[14,1,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3937c4ec-d6c7-4fb7-a30c-0046997de708",
   "metadata": {},
   "outputs": [],
   "source": [
    "counters = [0  for _ in range(MOVIE_COUNTS+1)]\n",
    "for i in range(len(movies)):\n",
    "    counters[movies[i][0]] = movies[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "016128af-ee7b-47f8-aa03-7fb681b09618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 10, 150]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8d0ed288-6c57-49f3-809b-73aa48d29672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 [0, 0, 150]\n",
      "160 [0, 0, 0]\n",
      "160 [0, 0, -20]\n",
      "160 [0, -5, -20]\n"
     ]
    }
   ],
   "source": [
    "answer = 0\n",
    "for i in range(len(tickets)):\n",
    "    if counters[tickets[i][1]] == 0 :\n",
    "        answer += min(counters[tickets[i][1]], tickets[i][2])\n",
    "        counters[tickets[i][1]] -= tickets[i][2]\n",
    "    elif counters[tickets[i][1]] > 0 :\n",
    "        answer += counters[tickets[i][1]] \n",
    "        counters[tickets[i][1]] = 0 \n",
    "        \n",
    "    print(answer, counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7aadda47-22b0-4e03-b95b-64c417faf87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da613eb-a2c6-4c6d-9330-b889087ea601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
